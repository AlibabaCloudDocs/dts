# 从自建Oracle迁移至阿里云消息队列Kafka

通过数据传输服务DTS（Data Transmission Service），您可以将自建Oracle迁移至阿里云消息队列Kafka版或自建Kafka，从而扩展消息处理能力。本文以自建Oracle数据库迁移至阿里云消息队列Kafka实例为例，介绍数据迁移任务的配置流程。

-   自建Oracle数据库的版本为9i、10g、11g、12c、18c或19c版本。
-   自建Oracle数据库已开启Supplemental Logging，且要求supplemental\_log\_data\_pk，supplemental\_log\_data\_ui已开启，详情请参见[Supplemental Logging](https://docs.oracle.com/database/121/SUTIL/GUID-D857AF96-AC24-4CA1-B620-8EA3DF30D72E.htm#SUTIL1582)。
-   自建Oracle数据库已开启ARCHIVELOG（归档模式），设置合理的归档日志保持周期，并且确保归档日志能够被访问，详情请参见[ARCHIVELOG](https://docs.oracle.com/database/121/ADMIN/archredo.htm#ADMIN008)。
-   自建Oracle数据库为源库时，您需要先执行相应的准备工作，详情请参见[准备工作概览]()。
-   自建Oracle数据库中的待迁移表需具备主键或非空唯一索引。
-   阿里云消息队列Kafka的版本为0.10.1.0~2.x，自建Kafka版本为0.10.1.0~2.7.0版本
-   目标Kafka实例的存储空间须大于自建Oracle数据库占用的存储空间。
-   目标Kafka实例中已创建用于接收同步数据的Topic，详情请参见[创建Topic](https://help.aliyun.com/document_detail/99952.html#title-7lc-gsy-p0n)[创建Topic](https://www.alibabacloud.com/help/zh/doc-detail/99952.html#title-7lc-gsy-p0n)。

## 注意事项

-   DTS在执行全量数据迁移时将占用源库和目标库一定的读写资源，可能会导致数据库的负载上升，在数据库性能较差、规格较低或业务量较大的情况下（例如源库有大量慢SQL、存在无主键表或目标库存在死锁等），可能会加重数据库压力，甚至导致数据库服务不可用。因此您需要在执行数据迁移前评估源库和目标库的性能，同时建议您在业务低峰期执行数据迁移（例如源库和目标库的CPU负载在30%以下）。
-   对于迁移失败的任务，DTS会触发自动恢复。在您将业务切换至目标库之前，请务必先停止或释放该任务，避免该任务被自动恢复，导致源端数据覆盖目标库的数据。
-   如果您的自建Oracle版本为12c及以上，待迁移表的名称长度需不超过30个字节。
-   如果源库中待迁移的表没有主键或唯一约束，且所有字段没有唯一性，可能会导致目标数据库中出现重复数据。

## 费用说明

|迁移类型|链路配置费用|公网流量费用|
|----|------|------|
|结构迁移和全量数据迁移|不收费。|通过公网将数据迁移出阿里云时将收费，详情请参见[产品定价]()。|
|增量数据迁移|收费，详情请参见[产品定价]()。|

## 迁移类型说明

|迁移类型|说明|
|----|--|
|结构迁移|DTS将源库中待迁移对象的结构定义迁移到目标库。当前场景DTS仅支持表结构迁移。|
|全量数据迁移|DTS将源库中待迁移对象的存量数据全部迁移至目标库。 **说明：** 在结构迁移和全量数据迁移完成之前，请勿对迁移对象执行DDL操作，否则可能导致迁移失败。 |
|增量数据迁移|DTS在全量数据迁移的基础上轮询并捕获自建Oracle数据库产生的redo log，将自建Oracle数据库的增量更新数据实时迁移至目标库。 通过增量数据迁移可以实现在自建应用不停服的情况下，平滑地完成数据迁移。在增量数据迁移阶段，DTS支持同步的DML、DDL语句。 |

## 消息格式

迁移到Kafka集群中的数据以avro格式存储，您需要根据avro schema定义进行数据解析，schema定义详情请参见[DTS avro schema定义](https://github.com/LioRoger/subscribe_example/tree/master/avro)。

## 准备工作

登录待迁移的Oracle数据库，创建用于采集数据的账号并授权。

**说明：** 如您已创建包含下述权限的账号，可跳过本步骤。

|数据库|结构迁移|全量迁移|增量数据迁移|
|:--|:---|:---|:-----|
|自建Oracle数据库|schema的owner权限|schema的owner权限|DBA|

数据库账号创建及授权方法：

自建Oracle数据库请参见[CREATE USER](https://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_8003.htm)和[GRANT](https://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_9013.htm)。

**说明：** 如需执行增量数据迁移，且不允许授予DBA权限，您可以参照以下内容为数据库账号授予更精细化的权限。



## 操作步骤

1.  登录[数据传输控制台](https://dts.console.aliyun.com/)[数据传输控制台](https://dts-intl.console.aliyun.com/)。

2.  在左侧导航栏，单击**数据迁移**。

3.  在**迁移任务列表**页面顶部，选择迁移的目标实例所属地域。

    ![选择地域](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/2767559951/p50439.png)

4.  单击页面右上角的**创建迁移任务**。

5.  配置迁移任务的源库和目标库连接信息。

    ![源库和目标库信息](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/5746574161/p244888.png)

    |类别|配置|说明|
    |:-|:-|:-|
    |无|任务名称|DTS会自动生成一个任务名称，建议配置具有业务意义的名称（无唯一性要求），便于后续识别。|
    |源库信息|实例类型|根据源库的部署位置进行选择，本文以**有公网IP的自建数据库**为例介绍配置流程。 **说明：** 当自建数据库为其他实例类型时，您还需要执行相应的准备工作，详情请参见[准备工作概览]()。 |
    |实例地区|当实例类型选择为**有公网IP的自建数据库**时，**实例地区**无需设置。 **说明：** 如果您的自建Oracle数据库进行了白名单安全设置，您需要在**实例地区**配置项后，单击**获取DTS IP段**来获取到DTS服务器的IP地址，并将获取到的IP地址加入自建Oracle数据库的白名单安全设置中。 |
    |数据库类型|选择**Oracle**。|
    |主机名或IP地址|填入自建Oracle数据库的访问地址，此处填入公网IP地址。|
    |端口|填入自建Oracle数据库的服务端口，默认为**1521**。 **说明：** 本案例中，该服务端口已开放至公网。 |
    |实例类型|    -   **非RAC实例**：选择该项后，您还需要填写**SID**信息。
    -   **RAC实例**：选择该项后，您还需要填写**ServiceName**信息。 |
    |数据库账号|填入自建Oracle的数据库账号，权限要求请参见[准备工作](#section_u0v_2me_cdv)。|
    |数据库密码|填入该数据库账号对应的密码。 **说明：** 源库信息填写完毕后，您可以单击**数据库密码**后的**测试连接**来验证填入的源库信息是否正确。源库信息填写正确则提示**测试通过**；如果提示**测试失败**，单击**测试失败**后的**诊断**，根据提示调整填写的源库信息。 |
    |目标库信息|实例类型|选择**通过专线/VPN网关/智能接入网关接入的自建数据库**。 **说明：** 由于DTS暂时不支持直接选择消息队列Kafka版，此处将其作为自建Kafka来配置数据同步。 |
    |实例地区|选择目标Kafka实例所属地域。|
    |已和源端数据库联通的VPC|选择目标Kafka实例所属的专有网络ID。您可以在Kafka实例的**基本信息**页面中查看到专有网络ID。![kafka_vpcid](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/5746574161/p244800.png) |
    |数据库类型|选择为**Kafka**。|
    |IP地址|填入Kafka实例**默认接入点**中的任意一个IP地址。 **说明：** 您可以在Kafka实例的**基本信息**页面中，获取**默认接入点**对应的IP地址。 |
    |端口|Kafka实例的服务端口，默认为9092。|
    |数据库账号|填入Kafka实例的用户名。 **说明：** 如果Kafka实例的实例类型为**VPC实例**，无需配置**数据库账号**和**数据库密码**。 |
    |数据库密码|填入该用户名对应的密码。|
    |Topic|单击右侧的**获取Topic列表**，然后在下拉框中选择具体的Topic。|
    |存储DDL的Topic|单击右侧的**获取Topic列表**，然后在下拉框中选择具体的Topic，用于存储DDL信息。如果未指定，DDL信息默认存储在**Topic**选择的Topic中。|
    |Kafka版本|根据Kafka实例版本，选择对应的版本信息。|
    |连接方式|根据业务及安全需求，选择**非加密连接**或**SCRAM-SHA-256**。|
    |是否使用Kafka Schema Registry|Kafka Schema Registry是元数据提供服务层，提供了一个RESTful接口，用于存储和检索Avro Schema。     -   **否**：不使用Kafka Schema Registry。
    -   **是**：使用Kafka Schema Registry。您需要输入Avro Schema在Kafka Schema Registry注册的URL或IP。 |

6.  配置完成后，单击页面右下角的**授权白名单并进入下一步**。

7.  配置迁移类型、策略和对象信息。

    ![迁移对象配置](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/5746574161/p244895.png)

    |配置|说明|
    |:-|:-|
    |迁移类型|同时选中**结构迁移**、**全量数据迁移**和**增量数据迁移**。**说明：** 如果未选中**增量数据迁移**，为保障数据一致性，全量数据迁移期间请勿在源库中写入新的数据。 |
    |投递到kafka的数据格式|迁移到Kafka集群中的数据以avro格式存储，您需要根据avro schema定义进行数据解析，schema定义详情请参见[DTS avro schema定义](https://github.com/LioRoger/subscribe_example/tree/master/avro)。|
    |迁移到Kafka Partition策略|根据业务需求选择迁移的策略，详细介绍请参见[Kafka Partition迁移策略说明](/intl.zh-CN/数据迁移/迁移任务管理/Kafka Partition迁移策略说明.md)。|
    |迁移对象|在迁移对象框中单击待迁移的表，然后单击![向右小箭头](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/8502659951/p40698.png)图标将其移动至已选择对象框。**说明：** DTS会自动将表名映射为步骤5选择的Topic名称。如需更换迁移的目标Topic，请参见[库表列映射](/intl.zh-CN/数据迁移/迁移任务管理/库表列映射.md)。 |
    |源、目标库无法连接重试时|默认重试12小时，您也可以自定义重试时间。如果DTS在设置的时间内重新连接上源、目标库，迁移任务将自动恢复。否则，迁移任务将失败。**说明：** 由于连接重试期间，DTS将收取任务运行费用，建议您根据业务需要自定义重试时间，或者在源和目标库实例释放后尽快释放DTS实例。 |
    |目标库对象名称大小写策略|您可以配置目标实例中迁移对象的库名、表名和列名的英文大小写策略。默认情况下选择**DTS默认策略**，您也可以选择与源库、目标库默认策略保持一致。更多信息，请参见[目标库对象名称大小写策略](/intl.zh-CN/数据迁移/迁移任务管理/目标库对象名称大小写策略.md)。|

8.  单击页面右下角的**预检查并启动**。

    **说明：**

    -   在数据迁移任务正式启动之前，会先进行预检查。只有预检查通过后，才能成功启动数据迁移任务。
    -   如果预检查失败，单击具体检查项后的![提示](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/8502659951/p47468.png)，查看失败详情。
        -   您可以根据提示修复后重新进行预检查。
        -   如无需修复告警检测项，您也可以选择**确认屏蔽**、**忽略告警项并重新进行预检查**，跳过告警检测项重新进行预检查。
9.  预检查通过后，单击**下一步**。

10. 在弹出的购买配置确认对话框，选择**链路规格**并选中**数据传输（按量付费）服务条款**。

11. 单击**购买并启动**，迁移任务正式开始。


## 结束迁移任务

**警告：** 为尽可能地减少业务切换带来的影响，您可以建立回退方案（将目标库的增量数据实时迁移回源库），详情请参见[业务切换流程](/intl.zh-CN/数据迁移/迁移任务管理/业务切换流程.md)。如果不涉及业务切换，您可以结束迁移任务。

-   全量数据迁移

    请勿手动结束迁移任务，否则可能导致数据不完整。您只需等待迁移任务完成即可，迁移任务会自动结束。

-   增量数据迁移

    迁移任务不会自动结束，您需要手动结束迁移任务。

    1.  观察迁移任务的进度变更为**增量迁移**，并显示为**无延迟**状态时，将源库停写几分钟，此时**增量迁移**的状态可能会显示延迟的时间。
    2.  等待迁移任务的**增量迁移**再次进入**无延迟**状态后，手动结束迁移任务。

        ![结束增量迁移任务](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6767559951/p47604.png)


用于数据迁移的数据库账号拥有读写权限，为保障数据库安全性，请在数据迁移完成后，删除自建Oracle数据库中用于数据迁移的数据库账号，以及修改目标Kafka实例中RAM用户的权限。具体步骤，请参见[RAM主子账号授权](/intl.zh-CN/权限控制/RAM主子账号授权.md)。

